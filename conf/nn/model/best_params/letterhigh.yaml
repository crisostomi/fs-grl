_target_: fs_grl.pl_modules.distance_metric_learning.DistanceMetricLearning

optimizer:
  _target_: torch.optim.Adam
  lr: 1e-2

#lr_scheduler:
#  _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
#  T_0: 10
#  T_mult: 2
#  eta_min: 0 # min value for the lr
#  last_epoch: -1
#  verbose: False

model:
  _target_: fs_grl.modules.baselines.tadam.TADAM
  feature_dim: ???
  num_classes: ???
  metric_scaling_factor: 90.0
  gamma_0_init: 0.0
  beta_0_init: 5.0

  loss_weights:
    classification_loss: 1
    latent_mixup_reg: 0.1
    intraclass_var_reg: 0.0
    film_reg: 0.3

  num_classes_per_episode: ${nn.data.train_episode_hparams.num_classes_per_episode}

  embedder:
    _target_: fs_grl.modules.graph_embedder.GraphEmbedder
    pooling:
      _target_: fs_grl.modules.pooling.sum.GlobalSumPool
    feature_dim: ???

    node_embedder:
      _target_: fs_grl.modules.node_embedder.NodeEmbedder
      feature_dim: ???
      embedding_dim: 32
      hidden_dim: ${nn.model.model.embedder.node_embedder.embedding_dim}
      num_convs: 3
      dropout_rate: 0.7
      do_preprocess: false
      conv_norm: torch_geometric.nn.GraphNorm
      postproc_mlp_norm: torch.nn.LayerNorm
      num_preproc_mlp_layers: 1
      num_postproc_mlp_layers: 2
      num_gin_mlp_layers: 2
      jump_mode: cat
      non_linearity:
        _target_: torch.nn.ReLU
